{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2982635,"sourceType":"datasetVersion","datasetId":1828076},{"sourceId":2950550,"sourceType":"datasetVersion","datasetId":1808997},{"sourceId":6039502,"sourceType":"datasetVersion","datasetId":3454656},{"sourceId":91702813,"sourceType":"kernelVersion"},{"sourceId":136055519,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport re\nimport nltk\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nfrom matplotlib import rc\nimport joblib\nimport json\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nfrom torch import nn,optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics.classification import F1Score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm.auto import tqdm\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:38:19.897352Z","iopub.status.idle":"2025-05-04T15:38:19.897855Z","shell.execute_reply.started":"2025-05-04T15:38:19.897694Z","shell.execute_reply":"2025-05-04T15:38:19.897711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# üì• Load & Prepare Arabic Summarization Dataset:\n**In this step, we load and clean our Arabic news summarization dataset. Here's what happens:**\n","metadata":{"execution":{"iopub.status.busy":"2025-05-01T17:48:37.716368Z","iopub.execute_input":"2025-05-01T17:48:37.716980Z","iopub.status.idle":"2025-05-01T17:48:37.721656Z","shell.execute_reply.started":"2025-05-01T17:48:37.716954Z","shell.execute_reply":"2025-05-01T17:48:37.720689Z"}}},{"cell_type":"code","source":"data = []\nxl_sum = pd.DataFrame(data, columns=['text', 'summary'])\nsecond = pd.read_csv('/kaggle/input/arabic-summarization-bbc-news/bbc_news_arabic_summarization.csv')\nsecond = second.drop(['id','url','title'],axis=1)\nxl_sum = pd.concat([xl_sum,second])\nnew_column_names = {\n    'text': 'paragraph',\n    'summary': 'summary',\n}\n\n# Rename the columns using the dictionary\nxl_sum = xl_sum.rename(columns=new_column_names)\nxl_sum.head()","metadata":{"execution":{"iopub.status.busy":"2025-05-04T15:38:19.898892Z","iopub.status.idle":"2025-05-04T15:38:19.899217Z","shell.execute_reply.started":"2025-05-04T15:38:19.899036Z","shell.execute_reply":"2025-05-04T15:38:19.899052Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üßπ Data Cleaning: Removing Duplicates\nProvides a summary of dataset statistics: number of non-null entries, etc. While .describe() mainly gives info on numeric columns, it's useful to confirm your dataset shape","metadata":{}},{"cell_type":"code","source":"column_name = 'paragraph'  \nxl_sum = xl_sum.drop_duplicates(subset=column_name)\ncolumn_name = 'summary'  \nxl_sum = xl_sum.drop_duplicates(subset=column_name)\nxl_sum.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:38:28.433704Z","iopub.execute_input":"2025-05-04T15:38:28.433960Z","iopub.status.idle":"2025-05-04T15:38:28.443002Z","shell.execute_reply.started":"2025-05-04T15:38:28.433936Z","shell.execute_reply":"2025-05-04T15:38:28.441984Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1487126596.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'paragraph'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxl_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxl_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'summary'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxl_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxl_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxl_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'xl_sum' is not defined"],"ename":"NameError","evalue":"name 'xl_sum' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"# üßº Text Preprocessing Functions for Arabic Summarization\nTo prepare our dataset for training, we define a set of functions for cleaning and standardizing Arabic text:","metadata":{}},{"cell_type":"code","source":"counter = 0\ndef delete_links(input_text):\n    pettern  = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))'''\n    out_text = re.sub(pettern, ' ', input_text)\n    return out_text\n\ndef delete_repeated_characters(input_text):\n    pattern  = r'(.)\\1{2,}'\n    out_text = re.sub(pattern, r\"\\1\\1\", input_text)\n    return out_text\n\ndef remove_extra_spaces(input_text):\n    replace = ' +'\n    out_text = re.sub(replace, \" \", input_text)\n    words = nltk.word_tokenize(out_text)\n    words = [word for word in words if word.isalpha()]\n    out_text = ' '.join(words)\n    return out_text\n\ndef replace_letters(input_text):\n    replace = {\"ÿ£\": \"ÿß\",\"ÿ©\": \"Ÿá\",\"ÿ•\": \"ÿß\",\"ÿ¢\": \"ÿß\",\"\": \"\"}\n    replace = dict((re.escape(k), v) for k, v in replace.items()) \n    pattern = re.compile(\"|\".join(replace.keys()))\n    out_text = pattern.sub(lambda m: replace[re.escape(m.group(0))], input_text)\n    return out_text\n\ndef clean_text(input_text):\n    replace = r'[^\\u0621-\\u064A\\u0660-\\u0669\\u06F0-\\u06F90-9]'\n    out_text = re.sub(replace, \" \", input_text)\n    #words = nltk.word_tokenize(out_text)\n    #words = [word for word in words if word.isalpha()]\n    #out_text = ' '.join(words)\n    return out_text\n\ndef remove_vowelization(input_text):\n    vowelization = re.compile(\"\"\" Ÿë|Ÿé|Ÿã|Ÿè|Ÿå|Ÿê|Ÿç|Ÿí|ŸÄ\"\"\", re.VERBOSE)\n    out_text = re.sub(vowelization, '', input_text)\n    return out_text\n\ndef delete_stopwords(input_text):\n    stop_words = set(nltk.corpus.stopwords.words(\"arabic\") + nltk.corpus.stopwords.words(\"english\"))\n    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n    tokens = tokenizer.tokenize(input_text)\n    wnl = nltk.WordNetLemmatizer()\n    lemmatizedTokens =[wnl.lemmatize(t) for t in tokens]\n    out_text = [w for w in lemmatizedTokens if not w in stop_words]\n    out_text = ' '.join(out_text)\n    return out_text\n\ndef stem_text(input_text):\n    st = ISRIStemmer()\n    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n    tokens = tokenizer.tokenize(input_text)\n    out_text = [st.stem(w) for w in tokens]\n    out_text = ' '.join(out_text)\n    return out_text\n\n\ndef text_prepare(input_text, ar_text):\n    global counter\n    counter +=1\n\n    #out_text = delete_links(input_text)\n    #out_text = delete_repeated_characters(out_text)\n    #out_text = delete_stopwords(input_text)\n    out_text = clean_text(input_text)\n    #out_text = remove_extra_spaces(out_text)\n    if(counter%100==0):\n        print(counter,'\\n',out_text)\n    return out_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:38:20.351019Z","iopub.execute_input":"2025-05-04T15:38:20.351507Z","iopub.status.idle":"2025-05-04T15:38:20.361910Z","shell.execute_reply.started":"2025-05-04T15:38:20.351480Z","shell.execute_reply":"2025-05-04T15:38:20.361161Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# üßπ Applying Preprocessing to the Dataset\n* Applies the text_prepare() function to both the paragraph and summary columns.\n\n* args=(True,) passes the ar_text parameter (not used in the function logic currently).\n\n* This step cleans and standardizes all text data in preparation for model training.","metadata":{}},{"cell_type":"code","source":"xl_sum['paragraph'] = xl_sum['paragraph'].apply(text_prepare, args=(True,))\nxl_sum['summary'] = xl_sum['summary'].apply(text_prepare, args=(True,))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:38:20.363282Z","iopub.execute_input":"2025-05-04T15:38:20.363500Z","iopub.status.idle":"2025-05-04T15:38:20.389686Z","shell.execute_reply.started":"2025-05-04T15:38:20.363483Z","shell.execute_reply":"2025-05-04T15:38:20.388769Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1897096763.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxl_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxl_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxl_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxl_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'xl_sum' is not defined"],"ename":"NameError","evalue":"name 'xl_sum' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"# üìä Splitting the Dataset\n    * Splits the dataset xl_sum into two parts:\n\n        * 90% for training (train)\n\n        * 10% for validation (val)\n\n    * random_state=42 ensures the split is reproducible ‚Äî using the same seed will always return the same split.\n\n","metadata":{}},{"cell_type":"code","source":"#train, val = train_test_split(xl_sum, test_size=0.1, random_state=42)\ntrain, val = train_test_split(xl_sum, test_size=0.1, random_state=42)\ntrain = train.sample(frac=0.5, random_state=42).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:38:20.390009Z","iopub.status.idle":"2025-05-04T15:38:20.390311Z","shell.execute_reply.started":"2025-05-04T15:38:20.390151Z","shell.execute_reply":"2025-05-04T15:38:20.390165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üì¶ SummaryDataset Class ‚Äì Custom Dataset for Model Input\n\nThis class extends torch.utils.data.Dataset to format and encode the data for training the summarization model.","metadata":{}},{"cell_type":"code","source":"class SummaryDataset(Dataset):\n    def __init__(\n        self,\n        data,\n        text_max_token_len = 3000,\n        summary_max_token_len = 400\n    ):\n        self.tokenizer = AutoTokenizer.from_pretrained(\"Jezia/AraBART-finetuned-wiki-ar\")\n\n        \n        self.data = data\n        self.text_max_token_len = text_max_token_len\n        self.summary_max_token_len = summary_max_token_len\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index: int):\n        data_row = self.data.iloc[index]\n\n        text = data_row['paragraph']\n\n        text_encoding = self.tokenizer(\n            text,\n            max_length=self.text_max_token_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            add_special_tokens=True,\n            return_tensors='pt'\n        )\n        summary_encoding = self.tokenizer(\n            data_row['summary'],\n            max_length=self.summary_max_token_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            add_special_tokens=True,\n            return_tensors='pt'\n        )\n        \n        labels = summary_encoding['input_ids']\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        \n        return dict(\n            input_ids=text_encoding['input_ids'].flatten(),\n            attention_mask=text_encoding['attention_mask'].flatten(),\n            labels=labels.flatten(),\n            decoder_attention_mask=summary_encoding['attention_mask'].flatten()\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:33:31.439581Z","iopub.execute_input":"2025-05-03T22:33:31.439848Z","iopub.status.idle":"2025-05-03T22:33:31.446127Z","shell.execute_reply.started":"2025-05-03T22:33:31.439830Z","shell.execute_reply":"2025-05-03T22:33:31.445452Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"\n# üì¶ SummaryDataModule ‚Äì PyTorch Lightning Data Module\n\nThis class is used to organize and manage data loading during training and validation with PyTorch Lightning.","metadata":{}},{"cell_type":"code","source":"class SummaryDataModule(pl.LightningDataModule):\n    def __init__(self,train_path,val_path,batch_size=12, text_max_token_len = 3000, summary_max_token_len = 400):\n        super().__init__()\n        self.train_path,self.val_path= train_path,val_path\n        self.batch_size = batch_size\n        self.text_max_token_len = text_max_token_len\n        self.summary_max_token_len = summary_max_token_len\n    \n    def setup(self,stage=None):\n        train = self.train_path\n        val = self.val_path\n        self.train_dataset = SummaryDataset(data=train,\n                                            text_max_token_len=self.text_max_token_len,\n                                            summary_max_token_len=self.summary_max_token_len)\n        self.val_dataset = SummaryDataset(data=val,\n                                          text_max_token_len=self.text_max_token_len,\n                                          summary_max_token_len=self.summary_max_token_len)\n    \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset,batch_size=self.batch_size,shuffle=True,num_workers=2)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_dataset,batch_size=self.batch_size,shuffle=False,num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:33:38.160126Z","iopub.execute_input":"2025-05-03T22:33:38.160367Z","iopub.status.idle":"2025-05-03T22:33:38.166139Z","shell.execute_reply.started":"2025-05-03T22:33:38.160353Z","shell.execute_reply":"2025-05-03T22:33:38.165275Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# ü§ñ AraBart ‚Äì Classe du Mod√®le de R√©sum√© Automatique\n\nCette classe encapsule le mod√®le AraBART pour l'entra√Ænement et la validation en utilisant PyTorch Lightning.","metadata":{}},{"cell_type":"code","source":"class AraBart(pl.LightningModule):\n    def __init__(self, lr=0.0001):\n        super().__init__()\n        self.lr = lr\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"Jezia/AraBART-finetuned-wiki-ar\")\n    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n        output = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            decoder_attention_mask=decoder_attention_mask\n        )\n        return output.loss, output.logits\n    \n    def configure_optimizers(self):\n        return optim.AdamW(self.parameters(), lr=self.lr)\n    \n    def training_step(self, batch, batch_size):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        decoder_attention_mask = batch['decoder_attention_mask']\n\n        loss, outputs = self(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=labels\n        )\n\n        return loss\n    \n    def validation_step(self, batch, batch_size):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        decoder_attention_mask = batch['decoder_attention_mask']\n\n        loss, outputs = self(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=labels\n        )\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:33:56.181262Z","iopub.execute_input":"2025-05-03T22:33:56.181544Z","iopub.status.idle":"2025-05-03T22:33:56.187915Z","shell.execute_reply.started":"2025-05-03T22:33:56.181525Z","shell.execute_reply":"2025-05-03T22:33:56.187215Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Installe gdown, un outil Python permettant de t√©l√©charger des fichiers √† partir de Google Drive via un lien direct.\n","metadata":{}},{"cell_type":"code","source":"!pip install gdown\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:35:25.824737Z","iopub.execute_input":"2025-05-03T22:35:25.825043Z","iopub.status.idle":"2025-05-03T22:35:28.820256Z","shell.execute_reply.started":"2025-05-03T22:35:25.825005Z","shell.execute_reply":"2025-05-03T22:35:28.819527Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"üîπ T√©l√©chargement des Poids du Mod√®le","metadata":{}},{"cell_type":"code","source":"\n# Install gdown (only needed the first time)\n!pip install -q gdown\n\n# Download the model_weights.pth file directly from Google Drive\n!gdown --id 1fCA-ECnHykbTaueCsfQ-N2mf2YboYI7A -O model_weights.pth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:35:06.956466Z","iopub.execute_input":"2025-05-03T22:35:06.957169Z","iopub.status.idle":"2025-05-03T22:35:20.899233Z","shell.execute_reply.started":"2025-05-03T22:35:06.957146Z","shell.execute_reply":"2025-05-03T22:35:20.898292Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1fCA-ECnHykbTaueCsfQ-N2mf2YboYI7A\nFrom (redirected): https://drive.google.com/uc?id=1fCA-ECnHykbTaueCsfQ-N2mf2YboYI7A&confirm=t&uuid=c91889de-5a88-4b02-b340-e6032f67582e\nTo: /kaggle/working/model_weights.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 557M/557M [00:05<00:00, 98.1MB/s]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"üîπ Chargement du Mod√®le Pr√©-entra√Æn√©\n  * Cr√©e une instance du mod√®le AraBart que tu as d√©fini pr√©c√©demment.\n\n  * Charge les poids pr√©-entra√Æn√©s dans le mod√®le avec load_state_dict, en sp√©cifiant weights_only=True pour ne charger que les poids, et non la structure compl√®te du mod√®l","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n# First Time only\n\ndm = SummaryDataModule(train_path=train,\n                 val_path = val,\n                 text_max_token_len = 1000,\n                 batch_size=2)\n\ntrained_model = AraBart()\n# Loading model weights with the 'weights_only=True' flag\ntrained_model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:35:35.909871Z","iopub.execute_input":"2025-05-03T22:35:35.910515Z","iopub.status.idle":"2025-05-03T22:35:38.277202Z","shell.execute_reply.started":"2025-05-03T22:35:35.910488Z","shell.execute_reply":"2025-05-03T22:35:38.276508Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"* Configure le trainer PyTorch Lightning avec 3 √©poques et l'entra√Ænement sur CPU (gpus=0).\n\n* Lance l‚Äôentra√Ænement en utilisant les donn√©es pr√©par√©es (dm), mais cette partie est comment√©e car le mod√®le est d√©j√† entra√Æn√©.","metadata":{}},{"cell_type":"code","source":"# Training\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=3, enable_checkpointing=False)\ntrainer.fit(trained_model,dm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:35:44.911565Z","iopub.execute_input":"2025-05-03T22:35:44.912287Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c471406f5b304adfaf5d71e5c3814937"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a29e45d47074e2481c2f2d1112c22b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b847697a064a5694793b64496c8ec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9973e747f18e44d0aadac59dc92df407"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12e44c004ac4e0b8acbcc01ab03baa6"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:19:24.744352Z","iopub.execute_input":"2025-05-03T15:19:24.744642Z","iopub.status.idle":"2025-05-03T15:19:24.788077Z","shell.execute_reply.started":"2025-05-03T15:19:24.744611Z","shell.execute_reply":"2025-05-03T15:19:24.787548Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:19:09.233297Z","iopub.execute_input":"2025-05-03T15:19:09.233557Z","iopub.status.idle":"2025-05-03T15:19:09.238095Z","shell.execute_reply.started":"2025-05-03T15:19:09.233541Z","shell.execute_reply":"2025-05-03T15:19:09.237304Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**üîπ Sauvegarde des Poids du Mod√®le**\n> Explication :\n\n   1.  trained_model.state_dict() : R√©cup√®re les poids et param√®tres du mod√®le (c'est-√†-dire l'√©tat actuel du mod√®le) sous forme de dictionnaire.\n\n   2. torch.save() : Sauvegarde ce dictionnaire dans un fichier. Ici, tu sauves les poids du mod√®le sous le nom model_weights.pth.","metadata":{}},{"cell_type":"code","source":"torch.save(trained_model.state_dict(), 'model_weights.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.750093Z","iopub.status.idle":"2025-05-01T17:43:47.750383Z","shell.execute_reply.started":"2025-05-01T17:43:47.750237Z","shell.execute_reply":"2025-05-01T17:43:47.750249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download JSON file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='model_weights.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.751555Z","iopub.status.idle":"2025-05-01T17:43:47.751768Z","shell.execute_reply.started":"2025-05-01T17:43:47.751676Z","shell.execute_reply":"2025-05-01T17:43:47.751685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n    * La fonction prend un texte comme entr√©e et utilise un mod√®le pr√©-entra√Æn√© AraBART pour g√©n√©rer un r√©sum√©.\n\n    * Elle v√©rifie si un GPU est disponible et d√©place le mod√®le et les donn√©es vers le p√©riph√©rique appropri√© (GPU ou CPU).\n\n    *  Le texte est tokenis√© avec des param√®tres qui d√©finissent la longueur maximale et d'autres options n√©cessaires √† la g√©n√©ration.\n\n    *  La g√©n√©ration du r√©sum√© est effectu√©e en utilisant la m√©thode generate() avec des options pour contr√¥ler la longueur, les r√©p√©titions et le processus de g√©n√©ration.\n\n    * Le r√©sum√© est ensuite d√©cod√© √† partir des IDs g√©n√©r√©s et renvoy√© sous forme de texte lisible.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"Jezia/AraBART-finetuned-wiki-ar\")\n\ntrained_model.freeze()\n\ndef summarizeText(text, mymodel):\n    # Check if a GPU is available and if not, use a CPU\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Move the model to the GPU\n    mymodel = mymodel.to(device)\n\n    text_encoding = tokenizer(\n        text,\n        max_length=1000,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        add_special_tokens=True,\n        return_tensors='pt'\n    )\n\n    # Move the text encoding to the GPU\n    text_encoding = {key: val.to(device) for key, val in text_encoding.items()}\n\n    generated_ids = mymodel.generate(\n        input_ids=text_encoding['input_ids'],\n        attention_mask=text_encoding['attention_mask'],\n        max_length=1000,\n        num_beams=5,\n        repetition_penalty=1.0,\n        length_penalty=0.8,\n        early_stopping=True\n    )\n\n    preds = [\n            tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n            for gen_id in generated_ids\n    ]\n\n    return \"\".join(preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.754649Z","iopub.status.idle":"2025-05-01T17:43:47.754932Z","shell.execute_reply.started":"2025-05-01T17:43:47.754776Z","shell.execute_reply":"2025-05-01T17:43:47.754785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge\nfrom rouge import Rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.755827Z","iopub.status.idle":"2025-05-01T17:43:47.756115Z","shell.execute_reply.started":"2025-05-01T17:43:47.755997Z","shell.execute_reply":"2025-05-01T17:43:47.756010Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**charger les donn√©es JSONL et les afficher sous forme de DataFrame :**","metadata":{}},{"cell_type":"code","source":"data = []\nwith open('/kaggle/input/arabic-summarization-bbc-news/arabic_test.jsonl', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nxl_test = pd.DataFrame(data, columns=['text', 'summary'])\nxl_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.757244Z","iopub.status.idle":"2025-05-01T17:43:47.757437Z","shell.execute_reply.started":"2025-05-01T17:43:47.757345Z","shell.execute_reply":"2025-05-01T17:43:47.757353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Charge un √©chantillon de 10% des donn√©es de test √† partir d'un fichier JSON.\n\n2. Pour chaque exemple, g√©n√®re un r√©sum√© du texte √† l'aide du mod√®le AraBART.\n\n3. Affiche le texte et le r√©sum√© g√©n√©r√© pour chaque exemple.\n\n4. Sauvegarde les r√©sultats (ID de l'exemple et r√©sum√©) dans un DataFrame pour analyse ou √©valuation.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Load the full dataset\ndata = []\nwith open('/kaggle/input/arabic-summarization-bbc-news/arabic_test.jsonl', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\n\n# Create DataFrame\nxl_test = pd.DataFrame(data, columns=['text', 'summary'])\n\n# Take only 10% of the dataset\nxl_test = xl_test.sample(frac=0.1, random_state=42).reset_index(drop=True)\n\n# ---- SUMMARIZATION LOOP STARTS HERE ----\n\nmodel_summaries = pd.DataFrame(columns=['example_id','summary'])\n\nfor i in range(len(xl_test)):\n    sample_row = xl_test.iloc[i]\n    text = sample_row['text']\n    summary = summarizeText(text, trained_model.model)\n    \n    print(\"Text\\n\", text, \"\\nSummary\\n\", summary, \"\\n\\n\")\n\n    new_row = pd.DataFrame({'example_id': [i], 'summary': [summary]})\n    model_summaries = pd.concat([model_summaries, new_row], ignore_index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.758382Z","iopub.status.idle":"2025-05-01T17:43:47.758651Z","shell.execute_reply.started":"2025-05-01T17:43:47.758540Z","shell.execute_reply":"2025-05-01T17:43:47.758551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**# INFERENCEEEEEEEEEEEEEEE TAKES TIME**","metadata":{}},{"cell_type":"code","source":"'''model_summaries = pd.DataFrame(columns=['example_id','summary'])\nfor i in range(len(xl_test)):\n    sample_row = xl_test.iloc[i]\n    text = sample_row['text']\n    summary = summarizeText(text, trained_model.model)\n    print(\"Text\\n\", text, \"\\nSummary\\n\", summary, \"\\n\\n\")\n    \n    # Use pd.concat instead of append\n    new_row = pd.DataFrame({'example_id': [i], 'summary': [summary]})\n    model_summaries = pd.concat([model_summaries, new_row], ignore_index=True)'''\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.759711Z","iopub.status.idle":"2025-05-01T17:43:47.759998Z","shell.execute_reply.started":"2025-05-01T17:43:47.759844Z","shell.execute_reply":"2025-05-01T17:43:47.759856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Chargement des donn√©es : Les donn√©es de test sont charg√©es √† partir d'un fichier JSONL, puis converties en un DataFrame contenant des textes et leurs r√©sum√©s correspondants.\n\n2. G√©n√©ration des r√©sum√©s : Le mod√®le est utilis√© pour g√©n√©rer des r√©sum√©s pour chaque texte dans le jeu de test.\n\n3. Calcul des scores Rouge-L : Le score Rouge-L est calcul√© pour chaque paire de r√©sum√© g√©n√©r√© et r√©sum√© de r√©f√©rence, puis affich√©.\n\n4. Calcul de la moyenne : La moyenne des scores Rouge-L est calcul√©e pour √©valuer la performance globale du mod√®le sur les r√©sum√©s g√©n√©r√©s.","metadata":{}},{"cell_type":"code","source":"# Create an instance of the Rouge object\nrouge = Rouge()\nsystem_summaries = model_summaries[\"summary\"].tolist()\nreference_summaries = xl_test[\"summary\"].tolist()\n\n# Calculate RougeL scores for the list of summaries\nscores = rouge.get_scores(system_summaries, reference_summaries)\n\n# Print the RougeL scores for each summary pair\nrouge_l_score = []\nfor i, score in enumerate(scores):\n    rouge_l_score.append(score['rouge-l']['f'])\n    print(\"RougeL Score for Summary\", i + 1, \":\", score['rouge-l']['f'])\n\nprint(sum(rouge_l_score)/len(rouge_l_score))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.761058Z","iopub.status.idle":"2025-05-01T17:43:47.761259Z","shell.execute_reply.started":"2025-05-01T17:43:47.761164Z","shell.execute_reply":"2025-05-01T17:43:47.761174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create a histogram\nplt.hist(rouge_l_score, bins=100, color='blue', edgecolor='black')\n\n# Add labels and title\nplt.xlabel('Rouge L Score')\nplt.ylabel('Frequency')\nplt.title('Distribution of Rouge L Scores')\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.762603Z","iopub.status.idle":"2025-05-01T17:43:47.762911Z","shell.execute_reply.started":"2025-05-01T17:43:47.762759Z","shell.execute_reply":"2025-05-01T17:43:47.762772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# pr√©parer votre environnement pour utiliser des mod√®les de sentence-transformers pour transformer des textes en vecteurs et effectuer des comparaisons ou des analyses s√©mantiques.","metadata":{}},{"cell_type":"code","source":"!pip install sentence_transformers\nfrom sentence_transformers import SentenceTransformer, util","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.764327Z","iopub.status.idle":"2025-05-01T17:43:47.764552Z","shell.execute_reply.started":"2025-05-01T17:43:47.764446Z","shell.execute_reply":"2025-05-01T17:43:47.764456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n**Le but principal de Sentence Transformers dans ce projet est de fournir une m√©thode pour √©valuer la qualit√© des r√©sum√©s g√©n√©r√©s par ton mod√®le de r√©sum√©.Ce mod√®le ne g√©n√®re pas de texte lui-m√™me, mais il t'aide √† mesurer √† quel point les r√©sum√©s g√©n√©r√©s sont proches des r√©sum√©s de r√©f√©rence, en termes de contenu s√©mantique.**\n","metadata":{}},{"cell_type":"code","source":"model = SentenceTransformer('stsb-roberta-large')\nsentence1 = model_summaries[\"summary\"][0]\nsentence2 = xl_test[\"summary\"][0]\n# encode sentences to get their embeddings\nembedding1 = model.encode(sentence1, convert_to_tensor=True)\nembedding2 = model.encode(sentence2, convert_to_tensor=True)\n# compute similarity scores of two embeddings\ncosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\nprint(\"Sentence 1:\", sentence1)\nprint(\"Sentence 2:\", sentence2)\nprint(\"Similarity score:\", cosine_scores.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.765443Z","iopub.status.idle":"2025-05-01T17:43:47.765685Z","shell.execute_reply.started":"2025-05-01T17:43:47.765581Z","shell.execute_reply":"2025-05-01T17:43:47.765592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system_summaries = xl_test[\"summary\"].values\nreference_summaries = model_summaries[\"summary\"].values\nsemantic_test = []\nfor i in range(len(system_summaries)):\n    sentence1 = system_summaries[i]\n    sentence2 = reference_summaries[i]\n    embedding1 = model.encode(sentence1, convert_to_tensor=True)\n    embedding2 = model.encode(sentence2, convert_to_tensor=True)\n    # compute similarity scores of two embeddings\n    cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n    semantic = cosine_scores.item()\n    semantic_test.append(semantic)\n    print(\"Test\",i,\"Similarity score: \", semantic)\nprint(sum(semantic_test)/len(semantic_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.766635Z","iopub.status.idle":"2025-05-01T17:43:47.766883Z","shell.execute_reply.started":"2025-05-01T17:43:47.766779Z","shell.execute_reply":"2025-05-01T17:43:47.766788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a histogram\nplt.hist(semantic_test, bins=100, color='blue', edgecolor='black')\n\n# Add labels and title\nplt.xlabel('Semantic Similarity Score')\nplt.ylabel('Frequency')\nplt.title('Distribution of Semantic Similarity Scores')\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T17:43:47.767387Z","iopub.status.idle":"2025-05-01T17:43:47.767666Z","shell.execute_reply.started":"2025-05-01T17:43:47.767484Z","shell.execute_reply":"2025-05-01T17:43:47.767492Z"}},"outputs":[],"execution_count":null}]}